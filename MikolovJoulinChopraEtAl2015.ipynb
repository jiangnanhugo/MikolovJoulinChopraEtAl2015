{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Structurally Constrained Recurrent Network (SCRN) Model\n",
    "#\n",
    "# This gives an implementation of the SCRN model given in Mikolov et al. 2015, arXiv:1412.7753 [cs.NE], \n",
    "# https://arxiv.org/abs/1412.7753 using Python and Tensorflow.\n",
    "#\n",
    "# This IPython Notebook provides an example of how to call the associated library of Python scripts.  \n",
    "# Mikolov et al. should be consulted to make sure of the correct hyperparameter values.\n",
    "#\n",
    "# Stuart Hagler, 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import math\n",
    "import sys\n",
    "\n",
    "# Local Imports\n",
    "sys.path.insert(0, 'py')\n",
    "from lstm import lstm_graph\n",
    "from read_data import read_data\n",
    "from scrn import scrn_graph\n",
    "from srn import srn_graph\n",
    "from tokens import text_elements_to_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Flags\n",
    "rnn_flg = 3      # 1 for SRN\n",
    "                 # 2 for LSTM\n",
    "                 # 3 for SCRN\n",
    "usecase_flg = 1  # 1 for predicting letters\n",
    "                 # 2 for predicting words with cutoff for infrequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network-specific hyperparameters\n",
    "if rnn_flg == 1:\n",
    "    \n",
    "    # Network hyperparameters\n",
    "    hidden_size = 110               # Dimension of the hidden vector\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    num_training_unfoldings = 10    # Number of training unfoldings\n",
    "    state_size = []                 # Dimension of the state vector\n",
    "    \n",
    "elif rnn_flg == 2:\n",
    "    \n",
    "    # Network hyperparameters\n",
    "    hidden_size = 110               # Dimension of the hidden vector\n",
    "    state_size = hidden_size        # Dimension of the state vector\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    num_training_unfoldings = 10    # Number of training unfoldings\n",
    "    \n",
    "elif rnn_flg == 3:\n",
    "    \n",
    "    # Network hyperparameters\n",
    "    alpha = 0.95                    #\n",
    "    hidden_size = 100               # Dimension of the hidden vector\n",
    "    state_size = 10                 # Dimension of the state vector\n",
    "\n",
    "    # Training hyperparameters\n",
    "    num_training_unfoldings = 50    # Number of training unfoldings\n",
    "    \n",
    "# General network hyperparameters\n",
    "word_frequency_cutoff = 50          # Cutoff for infrequent words for usecase_flg = 2\n",
    "\n",
    "# General training hyperparameters\n",
    "base_training_batch_size = 32       # Training batch size across all towers\n",
    "clip_norm = 1.25                    # Norm for gradient clipping\n",
    "learning_decay = 1/1.5              # Multiplier to decay the learn rate when required\n",
    "learning_rate = 0.05                # Initial learning rate\n",
    "momentum = 0.9                      # Momentum for training\n",
    "num_epochs = 50                     # Total number of epochs to run the algorithm\n",
    "num_validation_unfoldings = 50      # Number of validation unfoldings\n",
    "optimization_frequency = 5          # Number of unfoldings before optimization step\n",
    "summary_frequency = 500             # Summary information is displayed after training this many batches\n",
    "validation_batch_size = 32          # Validation batch size for each tower\n",
    "\n",
    "# Cluster\n",
    "num_gpus = 1                        # Number of GPUs available\n",
    "\n",
    "# Logging\n",
    "logdir = '/tmp/tensorflow/log/'\n",
    "\n",
    "# Data file\n",
    "filename = 'data/text8.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare training, validation, test data sets\n",
    "num_towers = num_gpus\n",
    "training_batch_size = base_training_batch_size // num_towers\n",
    "raw_data = read_data(usecase_flg, filename)\n",
    "data, dictionary, reverse_dictionary, vocabulary_size = text_elements_to_tokens(usecase_flg, raw_data, \n",
    "                                                                                word_frequency_cutoff)\n",
    "training_size = math.floor((9/11)*len(raw_data)/num_towers)\n",
    "validation_size = math.floor((1/11)*len(raw_data)/num_towers)\n",
    "testing_size = math.floor((1/11)*len(raw_data)/num_towers)\n",
    "training_text = []\n",
    "validation_text = []\n",
    "testing_text = []\n",
    "for i in range(num_towers):\n",
    "    training_text.append(data[i*training_size:(i+1)*training_size])\n",
    "    validation_text.append(data[num_towers*training_size + i*validation_size: \\\n",
    "                                num_towers*training_size + (i+1)*validation_size])\n",
    "    testing_text.append(data[num_towers*(training_size + validation_size) + i*testing_size: \\\n",
    "                             num_towers*(training_size + validation_size) + (i+1)*testing_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 28\n",
      "Initialized\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-dba523d0be5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[1;31m# Train graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m graph.train(learning_rate, learning_decay, momentum, clip_norm, num_epochs, summary_frequency, training_text, \n\u001b[0;32m---> 19\u001b[0;31m             validation_text, testing_text, logdir)\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\Stuart Hagler\\Documents\\Python\\MikolovJoulinChopraEtAl2015\\py\\base_rnn_graph.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, learning_rate, learning_decay, momentum, clip_norm, num_epochs, summary_frequency, training_text, validation_text, testing_text, logdir)\u001b[0m\n\u001b[1;32m    307\u001b[0m                 self._training_step(session, learning_rate, learning_decay, momentum, \n\u001b[1;32m    308\u001b[0m                                     \u001b[0mclip_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_batches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_writer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m                                     epoch, summary_frequency)\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[1;31m# Validation Step:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Stuart Hagler\\Documents\\Python\\MikolovJoulinChopraEtAl2015\\py\\base_rnn_graph.py\u001b[0m in \u001b[0;36m_training_step\u001b[0;34m(self, session, learning_rate, learning_decay, momentum, clip_norm, training_batches, training_writer, epoch, summary_frequency)\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_training_unfoldings\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                     \u001b[0mtraining_feed_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_training_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtower\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_batches_next\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtower\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_optimize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_training_summary\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_feed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[1;31m# Summarize current performance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Stuart Hagler\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Stuart Hagler\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Stuart Hagler\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Stuart Hagler\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Stuart Hagler\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Vocabulary Size: %d' % vocabulary_size)\n",
    "\n",
    "# Initialize graph\n",
    "if rnn_flg == 1:\n",
    "    # Use SRN\n",
    "    graph = srn_graph(num_gpus, hidden_size, state_size, vocabulary_size, num_training_unfoldings, \n",
    "                      num_validation_unfoldings, training_batch_size, validation_batch_size, optimization_frequency)\n",
    "elif rnn_flg == 2:\n",
    "    # Use LSTM\n",
    "    graph = lstm_graph(num_gpus, hidden_size, state_size, vocabulary_size, num_training_unfoldings, \n",
    "                       num_validation_unfoldings, training_batch_size, validation_batch_size, optimization_frequency)\n",
    "elif rnn_flg == 3:\n",
    "    # Use SCRN\n",
    "    graph = scrn_graph(num_gpus, alpha, hidden_size, state_size, vocabulary_size, num_training_unfoldings, \n",
    "                       num_validation_unfoldings, training_batch_size, validation_batch_size, optimization_frequency)\n",
    "    \n",
    "# Train graph\n",
    "graph.train(learning_rate, learning_decay, momentum, clip_norm, num_epochs, summary_frequency, training_text, \n",
    "            validation_text, testing_text, logdir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
